{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = TextBlob(\"Python is a high-level, general-purpose programming language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.tags # tags is the property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sloria/TextBlob/blob/dev/textblob/en/sentiments.py\n",
    "\n",
    "The polarity and subjectivity of each word is given based on this file\n",
    "https://github.com/sloria/TextBlob/blob/dev/textblob/en/en-sentiment.xml\n",
    "\n",
    "For some words there are multiple repetitions, depending on the context. It is not entirly clear how the polarity and the subjectivity are given in this case, but I think through averaging. It seems to happen for polarity in the following case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob('abrupt').sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same happens with sentences, where polarity and subjectivity are averaged over the tokens. Nonetheless there is a further mechanism for the analysis of sentences. The library takes into account negations, by checking previous words, do that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TextBlob('not good').sentiment)\n",
    "print('------------------------------------------------------------------------------------')\n",
    "print([TextBlob(t_).sentiment for t_ in TextBlob('not good').words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertisement = TextBlob(\"Python is a high-level, beautiful, amaizing, and general-purpose programming language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertisement.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = TextBlob(\"Python is a high-level, beautiful, amaizing, terrific, and general-purpose programming language.\")\n",
    "for w_ in sentence.words:\n",
    "    print(w_, ':', TextBlob(w_).sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One first step is\n",
    "\n",
    "**Decrease noise - acting gently on the data**: get rid of the most frequent words. Articles, propositions are typically not useful. This is partially integrated in the tf import of the dataset, as we specify the first index (default is 3).\n",
    "\n",
    "**Decrease noise - with the axes**: here we can use a library as TextBlob to identify the words with high subjectivity and polarity and keeping only those for the task.\n",
    "\n",
    "**Get rid of entire sentences** if the polarity and subjectivity on both single words and entire sentences is low, intuitively with good chance that part of text is not meaningful to the task.\n",
    "\n",
    "**The dictionary is extremely wide, tf allows to import only the first $k$ most frequent words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = (TextBlob('good').sentiment)\n",
    "print(sentiment)\n",
    "sentiment.polarity, sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding\n",
    "\n",
    "There is a rich literature on word embeddings. The idea is to download pretrained representations. One possibility is the corpus of words trained on google-news. This is huge, and hard to fit in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import gensim.models.keyedvectors as word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? word2vec.KeyedVectors.load_word2vec_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ = '/Users/vanessa/src/GoogleNews-vectors-negative300.bin'\n",
    "embed_map = word2vec.KeyedVectors.load_word2vec_format('/Users/vanessa/src/GoogleNews-vectors-negative300.bin',\n",
    "                                                      binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is using the glove-wiki-gigaword pretrained (Wikipedia 2014 + GigaWord5) representations\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "It consists on a word-embedding of dimension 300 for each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "#word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n",
    "#result = word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "#print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = word_vectors.similarity('woman', 'man')\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_embedding = word_vectors['woman']  # numpy vector of a word\n",
    "man_embedding = word_vectors['man']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "np.dot(woman_embedding, man_embedding) / (norm(woman_embedding) * norm(man_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe makes use of words count and statistics. GloVe is a log-bilinear model with a weighted least-squares objective. The main intuition underlying the model is the simple observation that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a model\n",
    "\n",
    "This is what suggested in this google guide (MLCC)\n",
    "https://developers.google.com/machine-learning/guides/text-classification/step-2-5\n",
    "in the case of samples/words < 1500 it is a common practice to follow the left right branch of the diagram.\n",
    "\n",
    "Another thing people do is to use predefined dictionaries https://developers.google.com/machine-learning/guides/text-classification/step-3 to embed the words depending on their meaning. This is similar to what TextBlob does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1**  \n",
    "https://keras.io/examples/imdb_cnn/\n",
    "\n",
    "Embedding  \n",
    "Dropout  \n",
    "CNNLayer  \n",
    "MaxPooling  \n",
    "Dense  \n",
    "Dropout + ReLU  \n",
    "Dense + Sigmoid\n",
    "\n",
    "**Example 2**  \n",
    "https://keras.io/examples/imdb_cnn_lstm/  \n",
    "Embedding  \n",
    "Dropout  \n",
    "CNNLayer  \n",
    "MaxPooling  \n",
    "LSTM  \n",
    "Dense + Sigmoid\n",
    "\n",
    "**Example 3**  \n",
    "https://keras.io/examples/imdb_fasttext/\n",
    "\n",
    "**Suggestion from Chollet**\n",
    "\n",
    "For small $n$ a transformation using the tf-idf representation, followed by a linear algorithm as logistic regression is suggested. https://developers.google.com/machine-learning/guides/text-classification/step-3\n",
    "\n",
    "***tf*** stands for term frequency  \n",
    "***idf*** stands for inverse document frequency\n",
    "\n",
    "Some words are articles, prepositions, are extremely frequent but not extremely informative. The idf term take into account this possibility, by normalizing over the frequency o having that word in other documents.\n",
    "\n",
    "Other representations exist: $N$-grams. The analysis at word level is consider a unigram. In the N-grams we consider sequence of consecutive words. These models in my option are more complex, but a sort of feature extractors (related to redundancy)? Here is an implementation of how TF-IDF words in scikit-learn.\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "\n",
    "$\\text{idf}(t) = \\log\\left(\\frac{1+n}{1+\\text{df}(t)}\\right) + 1$\n",
    "\n",
    "$n$, the number of documents  \n",
    "df$(t)$, the number of documents in the document set that contain term $t$. The resulting tf-idf vectors are then normalized by the Euclidean norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse.csr_matrix import todense\n",
    "denseX = X.todense()\n",
    "print(denseX.shape)\n",
    "print(denseX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(denseX, axis=-1)  # the vectors are normalized. Each document sums to one\n",
    "# so not to have dependency from the length of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ['Yes this is a first document again']\n",
    "\n",
    "vectorizer.transform(new_text).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if we can do something similar with a fixed dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_words = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=max_n_words,\n",
    "                                                                      index_from=0)\n",
    "\n",
    "word_index = imdb.get_word_index(path='imdb_word_index.json')\n",
    "\n",
    "reverse_word_index = dict(\n",
    "[(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "decoded_review = ' '.join(\n",
    "[reverse_word_index.get(i, '?') for i in train_data[123]])\n",
    "\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3941.36+1296.50+1037.20+2074.40+1037.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "12448/3*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary_ = dict(pd.Series(word_index).sort_values(ascending=True)[:max_n_words])\n",
    "dictionary = {k_: i_-1 for (k_, i_) in zip(dictionary_.keys(), dictionary_.values())}\n",
    "del dictionary_\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decoded_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.fit_transform([decoded_review]).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one possibility. We compute for each review the vector of features using the tf-idf transform and we ran on top of this a classifier, which can be linear or non linear.\n",
    "\n",
    "Another possibility is to use the embedding provided by GloVe and then run a classification algorithm (the network). We must keep in mind that in this case, we want invariance by order. We can do an embedding on single words, but then we need to put them all together independently from their order. One thing I have seen is the global pooling. At this point we have only 300 features, which are the different components in the embedding representation and we can run any classification algorithm on top of this. \n",
    "\n",
    "There is also the possibility of playing with N-grams but this is not totally clear to me.\n",
    "\n",
    "Considering one word at the time can limit the results. Just think to the previous case of sentiment analysis with textblob, 'not good' and 'very good' has very different meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters(object):\n",
    "    \"\"\" Add hyper-parameters in init so when you read a json, it will get updated as your latest code. \"\"\"\n",
    "    def __init__(self,\n",
    "                 learning_rate=5e-2,\n",
    "                 architecture='FC',\n",
    "                 nodes=128,\n",
    "                 epochs=500,\n",
    "                 batch_size=10,\n",
    "                 loss='cross_entropy',\n",
    "                 optimizer='sgd',\n",
    "                 lr_at_plateau=True,\n",
    "                 reduction_factor=None,\n",
    "                 validation_check=True):\n",
    "        \"\"\"\n",
    "        :param learning_rate: float, the initial value for the learning rate\n",
    "        :param architecture: str, the architecture types\n",
    "        :param epochs: int, the number of epochs we want to train\n",
    "        :param batch_size: int, the dimension of the batch size\n",
    "        :param loss: str, loss type, cross entropy or square loss\n",
    "        :param optimizer: str, the optimizer type.\n",
    "        :param lr_at_plateau: bool, protocol to decrease the learning rate.\n",
    "        :param reduction_factor, int, the factor which we use to reduce the learning rate.\n",
    "        :param validation_check: bool, if we want to keep track of validation loss as a stopping criterion.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.architecture = architecture\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_at_plateau = lr_at_plateau\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.validation_check = validation_check\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\" Here we save the dataset specific related to each experiment. The name of the dataset,\n",
    "    the scenario, if we modify the original dataset, and the dimensions of the input.\n",
    "    This is valid for the modified_MNIST_dataset, verify if it is going to be valid next\"\"\"\n",
    "    def __init__(self,\n",
    "                 removed_words=0,\n",
    "                 first_index=0,\n",
    "                 n_training=10):\n",
    "        \"\"\"\n",
    "        :param removed_words: float, percentage of removed words\n",
    "        :param first_index: int, all the more frequent words are removed\n",
    "        :param n_training: int, number of training examples\n",
    "        \"\"\"\n",
    "        self.removed_words = removed_words\n",
    "        self.first_index = first_index\n",
    "        self.n_training = n_training\n",
    "\n",
    "\n",
    "class Experiment(object):\n",
    "    \"\"\"\n",
    "    This class represents your experiment.\n",
    "    It includes all the classes above and some general\n",
    "    information about the experiment index.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 output_path,\n",
    "                 train_completed=False,\n",
    "                 hyper=None,\n",
    "                 dataset=None):\n",
    "        \"\"\"\n",
    "        :param id: index of output data folder\n",
    "        :param output_path: output directory\n",
    "        :param train_completed: bool, it indicates if the experiment has already been trained\n",
    "        :param hyper: instance of Hyperparameters class\n",
    "        :param dataset: instance of Dataset class\n",
    "        \"\"\"\n",
    "        if hyper is None:\n",
    "            hyper = Hyperparameters()\n",
    "        if dataset is None:\n",
    "            dataset = Dataset()\n",
    "\n",
    "        self.id = id\n",
    "        self.output_path = output_path\n",
    "        self.train_completed = train_completed\n",
    "        self.hyper = hyper\n",
    "        self.dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(0, output_path='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "    \"\"\" This class is meant to be the generator for different\n",
    "    types of transformation to the IMDb data. We load the IMDb\n",
    "    dataset and we apply different transformation, depending on the\n",
    "    Experiment attributes.\n",
    "    The training samples are extracted from the exp class\n",
    "    The validation samples per class are 5000 by default.\n",
    "    The test samples are the one in the original test set.\n",
    "    The extraction is such that we get a balanced dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 exp,\n",
    "                 n_vl=5000):\n",
    "        \"\"\" Initializer for the class. We pass the object Experiment to\n",
    "        assess the transformation required.\n",
    "        :param exp: Experiment object\n",
    "        :param n_vl: int, number of validation samples per class\n",
    "        \"\"\"\n",
    "        self.exp = exp\n",
    "        self.n_vl = n_vl\n",
    "\n",
    "        (tr_data, tr_labels), (ts_data, ts_labels) = imdb.load_data(num_words=5000,\n",
    "                                                                    index_from=0)\n",
    "        word_index = imdb.get_word_index(path='imdb_word_index.json')\n",
    "        self.reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "        self.glove_embedding = word_vectors\n",
    "        \n",
    "        # remove the indexes based on start_\n",
    "        id_tr, id_vl = self._split_train_validation(tr_labels)\n",
    "        \n",
    "        mean_tr, lst_tr = self.output_embedding([tr_data[i] for i in id_tr])\n",
    "        self.mean_tr = mean_tr\n",
    "        self.lst_tr = lst_tr\n",
    "        self.y_tr = tr_labels[id_tr]\n",
    "        \n",
    "        mean_vl, lst_vl = self.output_embedding([tr_data[i] for i in id_vl])\n",
    "        self.mean_vl = mean_vl\n",
    "        self.lst_vl = lst_vl\n",
    "        self.y_vl = tr_labels[id_vl]\n",
    "        \n",
    "        mean_ts, lst_ts = self.output_embedding([x_ts for x_ts in ts_data])\n",
    "        self.mean_ts = mean_ts\n",
    "        self.lst_ts = lst_ts\n",
    "        self.y_ts = ts_labels\n",
    "        \n",
    "\n",
    "    def _split_train_validation(self, y_learning):\n",
    "        \"\"\" Split of the training and validation set.\n",
    "        We chose randomly n_training elements from the learning set.\n",
    "        :param y_learning: labels from the training IMDb dataset.\n",
    "        \"\"\"\n",
    "        id_tr, id_vl = np.array([], dtype=int), np.array([], dtype=int)\n",
    "        n_tr = self.exp.dataset.n_training\n",
    "\n",
    "        for y_ in np.unique(y_learning):\n",
    "            id_class_y_ = np.where(y_learning == y_)[0]\n",
    "            tmp_id_tr = np.random.choice(id_class_y_,\n",
    "                                         size=n_tr,\n",
    "                                         replace=False)\n",
    "            tmp_id_vl = np.random.choice(np.setdiff1d(id_class_y_, tmp_id_tr),\n",
    "                                         size=self.n_vl,\n",
    "                                         replace=False)\n",
    "            id_tr = np.append(id_tr, tmp_id_tr)\n",
    "            id_vl = np.append(id_vl, tmp_id_vl)\n",
    "        return id_tr, id_vl\n",
    "\n",
    "    def output_embedding(self, X):\n",
    "        \"\"\" Dataset, it has dimensions (n, #words in sample i-th)\n",
    "        :param X: dataset, list of length n (samples), containing lists\n",
    "        :return mean_set: the mean embedding for each sample\n",
    "        :return lst_set: list containing the embedding for each word\n",
    "        \"\"\"\n",
    "        mean_set, lst_set = [], []\n",
    "        for x in X:\n",
    "            mean_, lst_ = self.preprocessing(x)  # x is a sample (containing n indexes)\n",
    "            mean_set.append(mean_)\n",
    "            lst_set.append(lst_)\n",
    "        return np.array(mean_set), lst_set\n",
    "        \n",
    "    def preprocessing(self, x):\n",
    "        \"\"\" Here we call the functions to perform different types of pre-processing.\n",
    "        This consists in:\n",
    "            1) excluding the most frequent words of the dictionary\n",
    "            2) remove a fixed amount of words\n",
    "            3) transform indexes into words\n",
    "            4) embed the each word and average\n",
    "        :param x: a sample, which is a list containing different indexes,\n",
    "        one for each word\n",
    "        \"\"\"\n",
    "        x = self._exclude_most_freq(x)\n",
    "        x = self._exclude_words(x)\n",
    "        x = self._index2str(x)\n",
    "        return self._embedding(x)\n",
    "    \n",
    "\n",
    "    def _exclude_most_freq(self, x):\n",
    "        \"\"\" We exclude the most frequent words here.\n",
    "        :param x: we pass one sample, a list containing indexes\n",
    "        \"\"\"\n",
    "        x = np.array(x)\n",
    "        return x[x >= self.exp.dataset.first_index]\n",
    "\n",
    "    def _exclude_words(self, x):\n",
    "        \"\"\" We exclude words here.\n",
    "        The most frequent ones are typically not relevant to the task.\n",
    "        :param x: a sample, it contains the indexes for the sample\n",
    "        \"\"\"\n",
    "        if self.exp.dataset.removed_words == 0:\n",
    "            return x\n",
    "        elif self.exp.dataset.removed_words >= 1:\n",
    "            raise ValueError(\"Maximum value of removed words must be less than one.\")\n",
    "        n_to_rm = int(self.exp.dataset.removed_words * len(x))\n",
    "        rnd_rm = np.random.choice(np.arange(len(x)), size=n_to_rm)\n",
    "        return list(np.delete(np.array(x), rnd_rm))\n",
    "\n",
    "    def _index2str(self, x):\n",
    "        \"\"\" From indices to words, given a single sample.\n",
    "        Transform in a list of string values.\n",
    "         :param x: a data from imdb.load_data(),\n",
    "         it contains the most used words\n",
    "         \"\"\"\n",
    "        return [self.reverse_word_index[id_] for id_ in x]\n",
    "\n",
    "    def _embedding(self, words_lst):\n",
    "        \"\"\" Here we generate the embedding for each sample.\n",
    "        We use the pre-trained word-vectors from gensim-data\n",
    "        :param words_lst: the list of words in a sample\n",
    "        :return embedding_mean: the mean value for the embedding\n",
    "        over the entire sample.\n",
    "        :return embedding_array: for each sample, this is a matrix,\n",
    "        each row is the embedding of a word in the dataset\n",
    "        \"\"\"\n",
    "        emb_lst = []\n",
    "        for w_ in words_lst:\n",
    "            if w_ in self.glove_embedding.vocab:\n",
    "                emb_lst.append(self.glove_embedding[w_])\n",
    "        embedding_array = np.array(emb_lst)\n",
    "        embedding_mean = np.mean(embedding_array, axis=0)\n",
    "        return embedding_mean, embedding_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = dataset_generator.lst_tr\n",
    "y_tr = dataset_generator.y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, GlobalMaxPooling2D, Concatenate\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array([x_.shape[0] for x_ in X_tr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_shapes = [[1, 3],[1, 2]]\n",
    "for b_ in bias_shapes:\n",
    "    print(b_)\n",
    "    \n",
    "# bias = [tf.zeros_initializer(shape=b_) for b_ in bias_shapes]\n",
    "bias = [tf.Variable(np.zeros((b_[0], b_[1]), dtype=np.float32)) for b_ in bias_shapes]\n",
    "weights = [tf.Variable(np.ones((b_[0], b_[1]), dtype=np.float32)) for b_ in bias_shapes]\n",
    "bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.maximum(weights[1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array([np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100)])\n",
    "\n",
    "dataset = dataset.reshape(9, 50, 100, 1)\n",
    "\n",
    "dataset = np.vstack((dataset,dataset))\n",
    "y__ = np.random.randint(low=0, high=2, size=18)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_reshape[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(filters=15,\n",
    "                 kernel_size=(8, 100),\n",
    "                 activation='relu',\n",
    "                 input_shape=(None, 100, 1),\n",
    "                 data_format='channels_last'))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(2, \n",
    "                activation='relu'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=X_tr_reshape, \n",
    "                    y=y_tr, \n",
    "                    epochs=5,\n",
    "                    batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_reshape = [] \n",
    "for x_ in X_tr:\n",
    "    w_, e_ = x_.shape\n",
    "    X_tr_reshape.append(x_.reshape(w_, e_, 1))\n",
    "X_tr_reshape = np.array(X_tr_reshape)\n",
    "\n",
    "X_tr_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "submodels = []\n",
    "FILTERS = 15\n",
    "MAX_LENGTH = 1000\n",
    "output_dims = 2\n",
    "\n",
    "\n",
    "for kw in [8, 12]:    # kernel sizes\n",
    "    submodel = Sequential()\n",
    "    submodel.add(Conv2D(FILTERS,\n",
    "                        kw,\n",
    "                        padding='valid',\n",
    "                        activation='relu',\n",
    "                        strides=1,\n",
    "                        input_shape=(None, 100, 1)))\n",
    "    submodel.add(GlobalMaxPooling2D())\n",
    "    submodels.append(submodel)\n",
    "    print(submodel.summary())\n",
    "\n",
    "# big_model = Sequential()\n",
    "big_model = submodel\n",
    "# big_model.add()\n",
    "big_model.add(Dense(1))\n",
    "\n",
    "print('Compiling model')\n",
    "big_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "history = big_model.fit(x=X_tr_reshape, \n",
    "                        y=np.array([[0]]), \n",
    "                        epochs=5,\n",
    "                        batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(Conv2D(filters=15,\n",
    "                 kernel_size=(8, 100),\n",
    "                 activation='relu',\n",
    "                 input_shape=(None, 100, 1)))\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(2, \n",
    "                activation='relu'))\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'] \n",
    "              )\n",
    "\n",
    "history = model.fit(X_tr_reshape, y__, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_reshape[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_tr_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_padded = np.zeros((len(X_tr), 1000, 100, 1))\n",
    "idx_bef_padding = [x_.shape[0] for x_ in X_tr]\n",
    "print(idx_bef_padding)\n",
    "for i_, x_ in enumerate(X_tr):\n",
    "    n_w_ = x_.shape[0]\n",
    "    X_data_padded[i_, :n_w_, :, :] = x_.reshape(n_w_, -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KERNEL_SIZE=8\n",
    "EMBEDDING_SIZE=100\n",
    "FILTERS=15\n",
    "MAX_LENGTH=1000\n",
    "\n",
    "np_mask = np.zeros((20, MAX_LENGTH-KERNEL_SIZE+1, 1, FILTERS))\n",
    "for i_, id_ in enumerate(idx_bef_padding):\n",
    "    np_mask[i_, :id_, 0, :] = 1\n",
    "\n",
    "# input_x = Input(shape=(MAX_LENGTH, EMBEDDING_SIZE, 1),\n",
    "#                 name='input_x')\n",
    "# input_mask = Input(shape=(MAX_LENGTH-KERNEL_SIZE+1, EMBEDDING_SIZE, 1),\n",
    "#                    name='input_m')\n",
    "\n",
    "print(input_mask)\n",
    "model = tf.\n",
    "\n",
    "print(model)\n",
    "model.add(Conv2D(filters=15,\n",
    "                 kernel_size=(KERNEL_SIZE, EMBEDDING_SIZE),\n",
    "                 activation='relu',\n",
    "                 input_shape=(MAX_LENGTH, EMBEDDING_SIZE, 1)))\n",
    "\n",
    "\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dense(2, \n",
    "                activation='relu'))\n",
    "\n",
    "sgd = SGD(learning_rate=0.6)\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# {\"x0\": x0, \"x1\": x1}\n",
    "history = model.fit({\"input_x\":X_data_padded, \n",
    "                     \"input_m\":np_mask}, \n",
    "                    y_tr, \n",
    "                    epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_array = np.random.choice(np.arange(1, 20), size=3)\n",
    "\n",
    "example = []\n",
    "for k_ in k_array:\n",
    "    example.append(np.random.randn(k_, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size=5\n",
    "kernels=2\n",
    "embedding=10\n",
    "output_classes=2\n",
    "\n",
    "def gen_padding_bm(data, window, kernels, max_len):\n",
    "    \"\"\" Here we create the input of the convolutional net. \n",
    "    There are two matrices which are passed as input,\n",
    "        the data, padded in such a way to have the shape of a tensor\n",
    "        the boolean mask, which is needed to reduce edge effect\n",
    "    :param data: this is a list, containing elements which are np.arrays\n",
    "    of dimension (number_of_words, size_of_the_embedding)\n",
    "    :param window: dimensionality of the kernel, number of words involved in the conv\n",
    "    :param kernels: number of filters\n",
    "    :param max_len: maximum dimension of the input\n",
    "    :returns data_padded: np.array of dimension (n_samples, max_len, emb, 1),\n",
    "    with 1 corresponding to the number of channels\n",
    "    :returns bool_mask: np.array of dimension (n_samples, max_len-window+1, emb, 1)\n",
    "    1 correspond to the features we want to keep\n",
    "    \"\"\"\n",
    "    n_samples = len(data)   # number of examples\n",
    "    _, emb = data[0].shape  # dimensionality of the embedding\n",
    "    data_padded = np.zeros((n_samples, max_len, emb, 1))  # uniform dim\n",
    "    bool_mask = np.zeros((n_samples, max_len-window+1, 1, kernels))  # bool mask\n",
    "    id_before_padding = [x_.shape[0] for x_ in data]\n",
    "    for i_, (n_w_, x_) in enumerate(zip(id_before_padding, data)):\n",
    "        data_padded[i_, :n_w_, :, :] = x_.reshape(n_w_, -1, 1)\n",
    "        bool_mask[i_, :n_w_-window, 0, :]  = 1  # to reduce edge effects\n",
    "    return data_padded, bool_mask\n",
    "\n",
    "def get_weight(shape, name):\n",
    "    \"\"\" Weights initializer.\n",
    "    :param shape: the shape of the model, list of dimensions for the layer\n",
    "    :param name: str, layer's name\n",
    "    \"\"\"\n",
    "    initializer = tf.initializers.glorot_uniform()\n",
    "    return tf.Variable(initializer(shape), name=name, trainable=True, dtype=tf.float32)\n",
    "\n",
    "shapes = [[window_size, embedding, 1, kernels],\n",
    "          [kernels, output_classes]]\n",
    "weights = [get_weight(shapes[i], 'weight{}'.format(i)) for i in range(len(shapes))]\n",
    "\n",
    "weights_init_npy0 = weights[0].numpy()\n",
    "weights_init_npy1 = weights[1].numpy()\n",
    "\n",
    "print('before training', weights[1])\n",
    "\n",
    "def loss(pred, target):\n",
    "    return tf.losses.categorical_crossentropy(target, pred)\n",
    "    \n",
    "def model(x, window=8, kernels=2, max_len=1000):\n",
    "    \"\"\" Here we generate the shallow CNN for text classification\n",
    "    as in the work of Yoon Kim\n",
    "    \n",
    "                https://arxiv.org/pdf/1408.5882.pdf\n",
    "                \n",
    "    We first reduce the dataset to a tensor of fixed dimensions.\n",
    "    We moreover generate a boolean array so to discard edge effects.\n",
    "    The architecture is such that\n",
    "    \n",
    "        padding, input tensor (n_samples, max_length, embedding, 1)\n",
    "        bool_mask, input (n_samples, max_length-window+1, 1, kernels)\n",
    "        \n",
    "        o1 = w1 * padding, dim: (n, max_length-window+1, 1, kernels)\n",
    "        o1 = o1 x bool_mask, dim: (n, max_length-window+1, 1, kernels)\n",
    "        o1 = max(o1), dim: (n, kernels)\n",
    "        o1 = w2 x o1, dim: (n, classes)\n",
    "        o1 = softmax(o1), dim: (n, classes) \n",
    "    \"\"\"\n",
    "    padded_data, bm = gen_padding_bm(x,\n",
    "                                     window=window_size,\n",
    "                                     kernels=kernels,\n",
    "                                     max_len=max_len)\n",
    "    embedding = padded_data.shape[2]\n",
    "    x1 = tf.cast(padded_data, dtype=tf.float32)  \n",
    "    x2 = tf.cast(bm, dtype=tf.float32) \n",
    "    c1 = tf.nn.conv2d(x1,\n",
    "                      weights[0], \n",
    "                      strides=1,\n",
    "                      padding=[[0,0],\n",
    "                               [0,0],\n",
    "                               [0,0],\n",
    "                               [0,0]])  \n",
    "    depad = tf.multiply(c1, x2)\n",
    "    max_pool_output = tf.math.reduce_max(depad, \n",
    "                                         axis=(1,2))\n",
    "    return tf.nn.softmax(tf.nn.relu(tf.matmul(max_pool_output, \n",
    "                                              weights[1])))\n",
    "\n",
    "optimizer = tf.optimizers.SGD(learning_rate=0.05, momentum=0)\n",
    "\n",
    "# almost but not exactly the same\n",
    "def train_step(model, inputs, outputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss(model(inputs), outputs)\n",
    "    grads = tape.gradient(current_loss, weights)\n",
    "    optimizer.apply_gradients(zip(grads, weights))\n",
    "    print(tf.reduce_mean(current_loss))\n",
    "    \n",
    "def train_sum_step(model, inputs, outputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss_sum_ = tf.reduce_mean(loss(model(inputs), outputs))\n",
    "    grads = tape.gradient(current_loss_sum_, weights)\n",
    "    optimizer.apply_gradients(zip(grads, weights))\n",
    "    print(current_loss_sum_)\n",
    "    \n",
    "num_epochs = 50\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_step(model, example, tf.one_hot(np.array([0, 1, 1]), depth=2))\n",
    "print('wo sum', weights[1])\n",
    "\n",
    "weights[0] = tf.Variable(weights_init_npy0)\n",
    "weights[1] = tf.Variable(weights_init_npy1)\n",
    "print('initialization', weights[1])\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_sum_step(model, example, tf.one_hot(np.array([0, 1, 1]), depth=2))\n",
    "print('w sum', weights[1])\n",
    "\n",
    "weights[0] = tf.Variable(weights_init_npy0)\n",
    "weights[1] = tf.Variable(weights_init_npy1)\n",
    "print('initialization', weights[1])\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    train_step(model, example, tf.one_hot(np.array([0, 1, 1]), depth=2))\n",
    "print('wo sum_ 2nd time', weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_epoch(n_samples, batch_size, outputs=None):\n",
    "    \"\"\" Generate the batches and call the train_step_batch function \"\"\"\n",
    "    batches_per_epoch = int(np.floor(n_samples / batch_size))\n",
    "    id_batches = np.random.choice(np.arange(n_samples),\n",
    "                                  size=(batches_per_epoch, batch_size))\n",
    "    for id_ in id_batches:\n",
    "        print(id_)\n",
    "    return id_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[1,2,3,4,5],[102,4,5,3],[2,4,5]]\n",
    "idx_ = np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice([0, 1], size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10 \n",
    "b = 2 \n",
    "\n",
    "X = np.random.randn(N, 3)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_batches = train_step_epoch(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[id_batches[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "\n",
    "class CNN_text:\n",
    "    \"Doc missing\"\n",
    "    def __init__(self,\n",
    "                 window_size,\n",
    "                 embedding,\n",
    "                 kernels,\n",
    "                 output_classes,\n",
    "                 max_length):\n",
    "        \"Doc missing\"\n",
    "        self.window_size = window_size\n",
    "        self.embedding = embedding\n",
    "        self.kernels = kernels\n",
    "        self.output_classes = output_classes\n",
    "        self.max_length = max_length\n",
    "    \n",
    "        self.shapes = [[self.window_size, self.embedding, 1, self.kernels],\n",
    "                       [self.kernels, self.output_classes]]\n",
    "        \n",
    "        weights = [get_weight(self.shapes[i], 'weight{}'.format(i)) \n",
    "                   for i in range(len(self.shapes))]\n",
    "        weights = self.weights\n",
    "        \n",
    "    def _gen_padding_bm(data, window, kernels, max_len):\n",
    "    \"\"\" Here we create the input of the convolutional net. \n",
    "    There are two matrices which are passed as input,\n",
    "        the data, padded in such a way to have the shape of a tensor\n",
    "        the boolean mask, which is needed to reduce edge effect\n",
    "    :param data: this is a list, containing elements which are np.arrays\n",
    "    of dimension (number_of_words, size_of_the_embedding)\n",
    "    :param window: dimensionality of the kernel, number of words involved in the conv\n",
    "    :param kernels: number of filters\n",
    "    :param max_len: maximum dimension of the input\n",
    "    :returns data_padded: np.array of dimension (n_samples, max_len, emb, 1),\n",
    "    with 1 corresponding to the number of channels\n",
    "    :returns bool_mask: np.array of dimension (n_samples, max_len-window+1, emb, 1)\n",
    "    1 correspond to the features we want to keep\n",
    "    \"\"\"\n",
    "    n_samples = len(data)   # number of examples\n",
    "    _, emb = data[0].shape  # dimensionality of the embedding\n",
    "    data_padded = np.zeros((n_samples, max_len, emb, 1))  # uniform dim\n",
    "    bool_mask = np.zeros((n_samples, max_len-window+1, 1, kernels))  # bool mask\n",
    "    id_before_padding = [x_.shape[0] for x_ in data]\n",
    "    for i_, (n_w_, x_) in enumerate(zip(id_before_padding, data)):\n",
    "        data_padded[i_, :n_w_, :, :] = x_.reshape(n_w_, -1, 1)\n",
    "        bool_mask[i_, :n_w_-window, 0, :]  = 1  # to reduce edge effects\n",
    "    return data_padded, bool_mask\n",
    " \n",
    "    def generate(x):\n",
    "    \"\"\" Here we generate the shallow CNN for text classification\n",
    "    as in the work of Yoon Kim\n",
    "    \n",
    "                https://arxiv.org/pdf/1408.5882.pdf\n",
    "                \n",
    "    We first reduce the dataset to a tensor of fixed dimensions.\n",
    "    We moreover generate a boolean array so to discard edge effects.\n",
    "    The architecture is such that\n",
    "    \n",
    "        padding, input tensor (n_samples, max_length, embedding, 1)\n",
    "        bool_mask, input (n_samples, max_length-window+1, 1, kernels)\n",
    "        \n",
    "        o1 = w1 * padding, dim: (n, max_length-window+1, 1, kernels)\n",
    "        o1 = o1 x bool_mask, dim: (n, max_length-window+1, 1, kernels)\n",
    "        o1 = max(o1), dim: (n, kernels)\n",
    "        o1 = w2 x o1, dim: (n, classes)\n",
    "        o1 = softmax(o1), dim: (n, classes) \n",
    "    \"\"\"\n",
    "    padded_data, bm = self._gen_padding_bm(x,\n",
    "                                           window=self.window_size,\n",
    "                                           kernels=self.kernels,\n",
    "                                           max_len=self.max_length)\n",
    "    embedding = padded_data.shape[2]\n",
    "    x1 = tf.cast(padded_data, dtype=tf.float32)  \n",
    "    x2 = tf.cast(bm, dtype=tf.float32) \n",
    "    c1 = tf.nn.conv2d(x1,\n",
    "                      weights[0], \n",
    "                      strides=1,\n",
    "                      padding=[[0,0],\n",
    "                               [0,0],\n",
    "                               [0,0],\n",
    "                               [0,0]])  \n",
    "    depad = tf.multiply(c1, x2)\n",
    "    max_pool_output = tf.math.reduce_max(depad, \n",
    "                                         axis=(1,2))\n",
    "    return tf.nn.softmax(tf.nn.relu(tf.matmul(max_pool_output, \n",
    "                                              weights[1])))\n",
    "\n",
    "    def loss(pred, target):\n",
    "        return tf.losses.categorical_crossentropy(target, pred)\n",
    "    \n",
    "    def get_weight(shape, name):\n",
    "        \"\"\" Weights initializer.\n",
    "        :param shape: the shape of the model, list of dimensions for the layer\n",
    "        :param name: str, layer's name\n",
    "        \"\"\"\n",
    "        initializer = tf.initializers.glorot_uniform()\n",
    "        return tf.Variable(initializer(shape), name=name, trainable=True, dtype=tf.float32)\n",
    "\n",
    "    def train_step(model, inputs, outputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            current_loss = loss(model(inputs), outputs)\n",
    "        grads = tape.gradient(current_loss, weights)\n",
    "        optimizer.apply_gradients(zip(grads, weights))\n",
    "        print(tf.reduce_mean(current_loss))\n",
    "    \n",
    "    def train_sum_step(model, inputs, outputs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            current_loss_sum_ = tf.reduce_mean(loss(model(inputs), outputs))\n",
    "        grads = tape.gradient(current_loss_sum_, weights)\n",
    "        optimizer.apply_gradients(zip(grads, weights))\n",
    "        print(current_loss_sum_)\n",
    "        \n",
    "    def learning_on_plateau():\n",
    "    \n",
    "    def early_stopping():\n",
    "        \n",
    "    num_epochs = 50\n",
    "    for e in range(num_epochs):\n",
    "        train_step(model, example, tf.one_hot(np.array([0, 1, 1]), depth=2))\n",
    "    print('wo sum', weights[1])\n",
    "\n",
    "    weights[0] = tf.Variable(weights_init_npy0)\n",
    "    weights[1] = tf.Variable(weights_init_npy1)\n",
    "    print('initialization', weights[1])\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "        train_sum_step(model, example, tf.one_hot(np.array([0, 1, 1]), depth=2))\n",
    "    print('w sum', weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight(shape, name):\n",
    "    \"\"\" Weights initializer.\n",
    "\n",
    "    :param shape: the shape of the model, list of dimensions for the layer\n",
    "    :param name: str, layer's name\n",
    "\n",
    "    :returns tf.Variable: a tensor object corresponding to a trainable layer\n",
    "    \"\"\"\n",
    "    initializer = tf.initializers.glorot_uniform()\n",
    "    return tf.Variable(initializer(shape),\n",
    "                       name=name,\n",
    "                       trainable=True,\n",
    "                       dtype=tf.float32)\n",
    "\n",
    "def _gen_padding_bm(data):\n",
    "    \"\"\" Here we create the input of the convolutional net.\n",
    "    There are two matrices which are passed as input,\n",
    "        the data, padded in such a way to have the shape of a tensor\n",
    "        the boolean mask, which is needed to reduce edge effect\n",
    "\n",
    "    :param data: this is a list, containing elements which are np.arrays\n",
    "    of dimension (number_of_words, size_of_the_embedding)\n",
    "\n",
    "    :returns data_padded: np.array of dimension (n_samples, max_len, emb, 1),\n",
    "    with 1 corresponding to the number of channels\n",
    "    :returns bool_mask: np.array of dimension (n_samples, max_len-window+1, emb, 1)\n",
    "    1 correspond to the features we want to keep\n",
    "    \"\"\"\n",
    "    n_samples = len(data)   # number of examples\n",
    "    _, emb = data[0].shape  # dimensionality of the embedding\n",
    "    max_length = np.max(np.array([d_.shape[0] for d_ in data]))\n",
    "    data_padded = np.zeros((n_samples, max_length, emb, 1))  # uniform dim\n",
    "    id_before_padding = [x_.shape[0] for x_ in data]\n",
    "\n",
    "    for i_, (n_w_, x_) in enumerate(zip(id_before_padding, data)):\n",
    "        data_padded[i_, :n_w_, :, :] = x_.reshape(n_w_, -1, 1)\n",
    "        \n",
    "    return data_padded  \n",
    "\n",
    "def train_batch_step(padded_data, bm_data, outputs):\n",
    "    \"\"\"\n",
    "    Here we train on a single batch\n",
    "\n",
    "    :param padded_data: np.array of uniform dimensions containing the embedding\n",
    "    :param bm_data: np.array with the bool mask to reduce the padding\n",
    "    :param outputs: output labels\n",
    "\n",
    "    :return: loss value over single batch\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss_sum_ = tf.reduce_mean(tf.losses.categorical_crossentropy(model(padded_data),\n",
    "                                                                              tf.one_hot(outputs,\n",
    "                                                                                         depth=2)))\n",
    "    grads = tape.gradient(current_loss_sum_, weights + bias)\n",
    "    optimizer.apply_gradients(zip(grads, weights + bias))\n",
    "    \n",
    "def train_epoch_step(inputs,\n",
    "                     outputs,\n",
    "                     batch_size):\n",
    "    \"\"\" Generate the batches and call the train_step_batch function.\n",
    "\n",
    "    :param inputs: not uniform input, a list\n",
    "    :param outputs: target, the labels\n",
    "    :param batch_size: number of example for the batch\n",
    "\n",
    "    :return [loss, accuracy]: loss and accuracy on the training set\n",
    "    \"\"\"\n",
    "    n_samples = len(inputs)  # this is a list\n",
    "    batches_per_epoch = int(np.floor(n_samples / batch_size))\n",
    "    id_batches = np.random.choice(np.arange(n_samples),\n",
    "                                  size=(batches_per_epoch, batch_size))\n",
    "    print('input dimensions', inputs_padding.shape)\n",
    "    for id_ in id_batches:\n",
    "        tmp_in = _gen_padding_bm([inputs_[i__] for i__ in id_])\n",
    "        train_batch_step(tmp_in, outputs[id_])\n",
    "    return evaluate(inputs, outputs)\n",
    "\n",
    "def model(x, padded=False): \n",
    "    \"\"\" We first reduce the dataset to a tensor of fixed dimensions.\n",
    "    We moreover generate a boolean array so to discard edge effects.\n",
    "    The architecture is such that\n",
    "\n",
    "        padding, input tensor (n_samples, max_length, embedding, 1)\n",
    "        bool_mask, input (n_samples, max_length-window+1, 1, kernels)\n",
    "\n",
    "        o1 = w1 * padding, dim: (n, max_length-window+1, 1, kernels)\n",
    "        o1 = o1 x bool_mask, dim: (n, max_length-window+1, 1, kernels)\n",
    "        o1 = max(o1), dim: (n, kernels)\n",
    "        o1 = w2 x o1, dim: (n, classes)\n",
    "        o1 = softmax(o1), dim: (n, classes)\n",
    "\n",
    "    :param x: input data\n",
    "    :param padded: bool, if True the data have already been padded\n",
    "\n",
    "    :returns logits: the output of the model, after applying the softmax function\n",
    "    \"\"\"\n",
    "    max_norm = 3  # for l2 regularization\n",
    "    bernoulli = 0.5  # \n",
    "    if not padded:\n",
    "        padded_data = _gen_padding_bm(x)\n",
    "    else:\n",
    "        padded_data = x\n",
    "\n",
    "    x1 = tf.cast(padded_data, dtype=tf.float32)\n",
    "\n",
    "    # for all the convolutional layers\n",
    "    c_lst = [tf.nn.conv2d(x1,\n",
    "                          weights[j_],\n",
    "                          strides=1,\n",
    "                          padding=[[0, 0],\n",
    "                                   [0, 0],\n",
    "                                   [0, 0],\n",
    "                                   [0, 0]])\n",
    "             for j_ in range(len(weights)-1)]\n",
    "    print([c_.shape for c_ in c_lst])\n",
    "\n",
    "    depad_lst = [tf.nn.relu(c_ + tf.constant(np.ones((c_.shape[0], c_.shape[1], c_.shape[2], 1),\n",
    "                                                      dtype=np.float32)) * b_)\n",
    "                 for (c_, b_) in zip(c_lst, bias[:-1])]\n",
    "    print([d_.shape for d_ in depad_lst])\n",
    "\n",
    "    max_pool_output = [tf.math.reduce_max(depad_, axis=(1, 2)) for depad_ in depad_lst]\n",
    "    print([m_pool.shape for m_pool in max_pool_output])\n",
    "    stack_output = tf.concat(max_pool_output, axis=-1)\n",
    "    \n",
    "    norm_weights = tf.norm(weights[-1], axis=-1)\n",
    "    bm = norm_weights > max_norm     \n",
    "        \n",
    "    l2_weights = tf.stack([max_norm * weights[-1][i] / norm_weights[i] \n",
    "                          if tmp_bm_ else weights[-1][i] for i, tmp_bm_ in enumerate(bm)])\n",
    "    \n",
    "    weights[-1] = tf.cast(l2_weights, dtype=tf.float32)\n",
    "    # weights[-1] l2 reg by heart\n",
    "    lst_layer = tf.matmul(tf.nn.dropout(stack_output, rate=bernoulli), weights[-1])\n",
    "    \n",
    "    print('shape weights', weights[-1].shape)\n",
    "    print(weights[-1])\n",
    "\n",
    "    return tf.nn.softmax(tf.nn.relu(tf.matmul(stack_output,\n",
    "                                              weights[-1]) + bias[-1]))\n",
    "\n",
    "\n",
    "window_size = [3, 4, 5]\n",
    "embedding = 15\n",
    "kernels = 100       # kernels for each convolutional filter?\n",
    "output_classes = 2  # task of sentiment analysis\n",
    "lr = 0.2\n",
    "\n",
    "shapes_conv_layer = [[w_, embedding, 1, kernels] for w_ in window_size]\n",
    "shapes_last_layer = [[kernels * len(window_size), output_classes]]\n",
    "shapes = shapes_conv_layer + shapes_last_layer\n",
    "\n",
    "weights = [get_weight(shapes[i], 'weight{}'.format(i))\n",
    "           for i in range(len(shapes))]\n",
    "\n",
    "bias_shapes = [[1, kernels] for w_ in range(len(window_size))] + [[1, output_classes]]\n",
    "print(bias_shapes)\n",
    "bias = [tf.Variable(np.zeros((b_[0], b_[1]), dtype=np.float32))\n",
    "        for b_ in bias_shapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5], [39, 3, 3, 2, 1]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = [[1,2,3,4,5], [4,3,2], [39,3,3,2,1]]\n",
    "[xx[i] for i in np.array([0,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xx[i])\n",
    "tf.minimum(3, tf.Variable(xx[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.array([np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100),\n",
    "                    np.random.randn(50, 100)])\n",
    "\n",
    "dataset = dataset.reshape(9, 50, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=int64, numpy=array([3, 3, 3])>\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=int64, numpy=\n",
      "array([[1, 2, 3],\n",
      "       [1, 0, 0]])>\n",
      "tf.Tensor(\n",
      "[[3 6 9]\n",
      " [3 0 0]], shape=(2, 3), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(3 * np.ones(3, dtype=np.int64))\n",
    "y = tf.Variable(np.array([[1,2,3],\n",
    "                          [1,0,0]]))\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "out_ = tf.math.multiply(tf.expand_dims(x, axis=0), y)\n",
    "\n",
    "print(out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3 6 9], shape=(3,), dtype=int64)\n",
      "tf.Tensor([4.2426405 6.        9.       ], shape=(3,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=239, shape=(3,), dtype=float32, numpy=array([0.7071068 , 0.5       , 0.33333334], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(out_[0])\n",
    "\n",
    "print(tf.norm(tf.cast(out_, dtype=tf.float32), axis=0))\n",
    "\n",
    "3 / tf.maximum(3, tf.norm(tf.cast(out_, dtype=tf.float32), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm = 3\n",
    "norm_weights = max_norm / tf.maximum(max_norm, tf.norm(out_[0]))\n",
    "\n",
    "out_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.multiply(tf.Variable(np.ones(3)), tf.Variable(np.ones(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 50, 100, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = x if padded else self._gen_padding_bm(x)\n",
    "max_norm = 3\n",
    "bernoulli = 0.5\n",
    "x1 = tf.cast(padded_data, dtype=tf.float32)\n",
    "c_lst = [tf.nn.conv2d(x1,\n",
    "                      self.weights[j_],\n",
    "                      strides=1,\n",
    "                      padding=[[0, 0],\n",
    "                               [0, 0],\n",
    "                               [0, 0],\n",
    "                               [0, 0]])\n",
    "         for j_ in range(len(self.weights)-1)]\n",
    "depad_lst = [tf.nn.relu(c_ + tf.constant(np.ones((c_.shape[0], c_.shape[1], c_.shape[2], 1),\n",
    "                                                  dtype=np.float32)) * b_)\n",
    "             for (c_, b_) in zip(c_lst, self.bias[:-1])]\n",
    "\n",
    "max_pool_output = [tf.math.reduce_max(depad_, axis=(1, 2)) for depad_ in depad_lst]\n",
    "stack_output = tf.concat(max_pool_output, axis=-1)  # max pooling for all filters\n",
    "norm_weights = max_norm / tf.maximum(max_norm, tf.norm(self.weights[-1], axis=0))\n",
    "weights = tf.math.multiply(self.weights[-1], tf.expand_dims(norm_weights, axis=0))\n",
    "\n",
    "return tf.nn.softmax(tf.nn.relu(tf.matmul(tf.nn.dropout(stack_output, rate=bernoulli),\n",
    "                                          weights)\n",
    "                                + self.bias[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
