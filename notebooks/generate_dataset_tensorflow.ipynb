{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters(object):\n",
    "    \"\"\" Add hyper-parameters in init so when you read a json, it will get updated as your latest code. \"\"\"\n",
    "    def __init__(self,\n",
    "                 learning_rate=5e-2,\n",
    "                 architecture=None,\n",
    "                 epochs=500,\n",
    "                 batch_size=10,\n",
    "                 loss='cross_entropy',\n",
    "                 optimizer='sgd',\n",
    "                 lr_at_plateau=True,\n",
    "                 reduction_factor=None,\n",
    "                 validation_check=True):\n",
    "        \"\"\"\n",
    "        :param learning_rate: float, the initial value for the learning rate\n",
    "        :param architecture: str, the architecture types\n",
    "        :param epochs: int, the number of epochs we want to train\n",
    "        :param batch_size: int, the dimension of the batch size\n",
    "        :param loss: str, loss type, cross entropy or square loss\n",
    "        :param optimizer: str, the optimizer type.\n",
    "        :param lr_at_plateau: bool, protocol to decrease the learning rate.\n",
    "        :param reduction_factor, int, the factor which we use to reduce the learning rate.\n",
    "        :param validation_check: bool, if we want to keep track of validation loss as a stopping criterion.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.architecture = architecture\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_at_plateau = lr_at_plateau\n",
    "        self.reduction_factor = reduction_factor\n",
    "        self.validation_check = validation_check\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\" Here we save the dataset specific related to each experiment. The name of the dataset,\n",
    "    the scenario, if we modify the original dataset, and the dimensions of the input.\n",
    "    This is valid for the modified_MNIST_dataset, verify if it is going to be valid next\"\"\"\n",
    "    # TODO: add output_dims\n",
    "    def __init__(self,\n",
    "                 dataset_name='dataset_1',\n",
    "                 scenario=1,\n",
    "                 additional_dims=2,\n",
    "                 n_training=10,\n",
    "                 redundancy_amount=None):\n",
    "        \"\"\"\n",
    "        :param dataset_name: str, dataset name\n",
    "        :param scenario: int, the learning paradigm\n",
    "        :param additional_dims: int, additional noise\n",
    "        :param n_training: int, number of training examples\n",
    "        :param redundancy_amount, percentage of redundant features, scenario 4 only\n",
    "        \"\"\"\n",
    "        self.dataset_name = dataset_name\n",
    "        self.scenario = scenario\n",
    "        self.additional_dims = additional_dims\n",
    "        self.n_training = n_training\n",
    "        self.redundancy_amount = redundancy_amount\n",
    "\n",
    "\n",
    "class Experiment(object):\n",
    "    \"\"\"\n",
    "    This class represents your experiment.\n",
    "    It includes all the classes above and some general\n",
    "    information about the experiment index.\n",
    "    IF YOU ADD ANOTHER CLASS, MAKE SURE TO INCLUDE IT HERE.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 id,\n",
    "                 output_path,\n",
    "                 train_completed=False,\n",
    "                 hyper=None,\n",
    "                 dataset=None):\n",
    "        \"\"\"\n",
    "        :param id: index of output data folder\n",
    "        :param output_path: output directory\n",
    "        :param train_completed: bool, it indicates if the experiment has already been trained\n",
    "        :param hyper: instance of Hyperparameters class\n",
    "        :param dataset: instance of Dataset class\n",
    "        \"\"\"\n",
    "        if hyper is None:\n",
    "            hyper = Hyperparameters()\n",
    "        if dataset is None:\n",
    "            dataset = Dataset()\n",
    "\n",
    "        self.id = id\n",
    "        self.output_path = output_path\n",
    "        self.train_completed = train_completed\n",
    "        self.hyper = hyper\n",
    "        self.dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "idx_start_dct = dict([(1, 0), (2, 30), (4, 15)])\n",
    "\n",
    "class DatasetGenerator:\n",
    "    \"\"\" This class is meant to be the generator for different\n",
    "    types of transformation to the IMDb datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 exp,\n",
    "                 n_vl=5000):\n",
    "        \"\"\" Initializer for the class. We pass the object Experiment to\n",
    "        assess the transformation required.\n",
    "        :param exp: Experiment object\n",
    "        :param n_vl: int, number of validation samples per class\n",
    "        \"\"\"\n",
    "        self.exp = exp\n",
    "        self.n_vl = n_vl\n",
    "\n",
    "        (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=5000,\n",
    "                                                                              index_from=idx_start_dct[self.exp.dataset.scenario])\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        word_index = imdb.get_word_index(path='imdb_word_index.json')\n",
    "        self.reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "        self.glove_embedding = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "    def split_train_validation(self):\n",
    "        \"\"\" Split of the training and validation set. \"\"\"\n",
    "        y_tr, y_vl, id_tr, id_vl = np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int), np.array([], dtype=int)\n",
    "        n_tr = self.exp.dataset.n_training\n",
    "\n",
    "        for y_ in np.unique(self.train_labels):\n",
    "            id_class_y_ = np.where(self.train_labels == y_)[0]\n",
    "            tmp_id_tr = np.random.choice(id_class_y_,\n",
    "                                         size=n_tr,\n",
    "                                         replace=False)\n",
    "            tmp_id_vl = np.random.choice(np.setdiff1d(id_class_y_, tmp_id_tr),\n",
    "                                         size=self.n_vl,\n",
    "                                         replace=False)\n",
    "\n",
    "            id_tr = np.append(id_tr, tmp_id_tr)\n",
    "            id_vl = np.append(id_vl, tmp_id_vl)\n",
    "            y_tr = np.append(y_tr, y_ * np.ones(n_tr))\n",
    "            y_vl = np.append(y_vl, y_ * np.ones(self.n_vl))\n",
    "\n",
    "        return id_tr, y_tr, id_vl, y_vl\n",
    "\n",
    "    def index2str(self, x):\n",
    "        \"\"\" Transform in a list of string values.\n",
    "         :param x: a data from imdb.load_data(),\n",
    "         it contains the most used words.\n",
    "         \"\"\"\n",
    "        return [self.reverse_word_index[id_] for id_ in x]\n",
    "\n",
    "    def embedding(self, words_lst):\n",
    "        \"\"\" Here we generate the embedding for each sample.\n",
    "        We use the pre-trained word-vectors from gensim-data\n",
    "        :param words_lst: the list of words in a sample\n",
    "        \"\"\"\n",
    "        embedding_array = [self.glove_embedding[w_] for w_ in words_lst]\n",
    "        embedding_mean = np.mean(np.array(embedding_array), axis=0)\n",
    "        return embedding_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp = Experiment(0, '.')\n",
    "exp.dataset.n_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generator = DatasetGenerator(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tr, _, id_vl, _ = dataset_generator.split_train_validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22532,  7581,  4532,  8789, 19070,  1138,  1848, 10136, 23842,\n",
       "       19887,  8170, 12955, 24824,  3255,  3007, 21461,  9576,  2369,\n",
       "        6542, 16761])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_words = dataset_generator.index2str(dataset_generator.train_data[123])\n",
    "\n",
    "len(lst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=5000,\n",
    "                                                                      index_from=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 304,\n",
       " 2,\n",
       " 1298,\n",
       " 17,\n",
       " 1023,\n",
       " 2508,\n",
       " 84,\n",
       " 2772,\n",
       " 49,\n",
       " 113,\n",
       " 2,\n",
       " 28,\n",
       " 4,\n",
       " 1,\n",
       " 88,\n",
       " 1217,\n",
       " 99,\n",
       " 10,\n",
       " 25,\n",
       " 107,\n",
       " 8,\n",
       " 3,\n",
       " 134,\n",
       " 10,\n",
       " 112,\n",
       " 216,\n",
       " 138,\n",
       " 32,\n",
       " 218,\n",
       " 953,\n",
       " 51,\n",
       " 10,\n",
       " 13,\n",
       " 8,\n",
       " 2711,\n",
       " 58,\n",
       " 319,\n",
       " 420,\n",
       " 9,\n",
       " 35,\n",
       " 73,\n",
       " 56,\n",
       " 1800,\n",
       " 69,\n",
       " 5,\n",
       " 2,\n",
       " 20,\n",
       " 2,\n",
       " 964,\n",
       " 9,\n",
       " 35,\n",
       " 82,\n",
       " 59,\n",
       " 355,\n",
       " 96]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=5000,\n",
    "                                                                      index_from=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 404,\n",
       " 102,\n",
       " 1398,\n",
       " 117,\n",
       " 1123,\n",
       " 2608,\n",
       " 184,\n",
       " 2872,\n",
       " 149,\n",
       " 213,\n",
       " 102,\n",
       " 128,\n",
       " 104,\n",
       " 101,\n",
       " 188,\n",
       " 1317,\n",
       " 199,\n",
       " 110,\n",
       " 125,\n",
       " 207,\n",
       " 108,\n",
       " 103,\n",
       " 234,\n",
       " 110,\n",
       " 212,\n",
       " 316,\n",
       " 238,\n",
       " 132,\n",
       " 318,\n",
       " 1053,\n",
       " 151,\n",
       " 110,\n",
       " 113,\n",
       " 108,\n",
       " 2811,\n",
       " 158,\n",
       " 419,\n",
       " 520,\n",
       " 109,\n",
       " 135,\n",
       " 173,\n",
       " 156,\n",
       " 1900,\n",
       " 169,\n",
       " 105,\n",
       " 2,\n",
       " 120,\n",
       " 102,\n",
       " 1064,\n",
       " 109,\n",
       " 135,\n",
       " 182,\n",
       " 159,\n",
       " 455,\n",
       " 196]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 404,\n",
       " 102,\n",
       " 1398,\n",
       " 117,\n",
       " 1123,\n",
       " 2608,\n",
       " 184,\n",
       " 2872,\n",
       " 149,\n",
       " 213,\n",
       " 102,\n",
       " 128,\n",
       " 104,\n",
       " 101,\n",
       " 188,\n",
       " 1317,\n",
       " 199,\n",
       " 110,\n",
       " 125,\n",
       " 207,\n",
       " 108,\n",
       " 103,\n",
       " 234,\n",
       " 110,\n",
       " 212,\n",
       " 316,\n",
       " 238,\n",
       " 132,\n",
       " 318,\n",
       " 1053,\n",
       " 151,\n",
       " 110,\n",
       " 113,\n",
       " 108,\n",
       " 2811,\n",
       " 158,\n",
       " 419,\n",
       " 520,\n",
       " 109,\n",
       " 135,\n",
       " 173,\n",
       " 156,\n",
       " 1900,\n",
       " 169,\n",
       " 105,\n",
       " 2,\n",
       " 120,\n",
       " 102,\n",
       " 1064,\n",
       " 109,\n",
       " 135,\n",
       " 182,\n",
       " 159,\n",
       " 455,\n",
       " 196]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
