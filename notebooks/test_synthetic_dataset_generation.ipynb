{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DatasetGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-816eb3431b09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mDG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatasetGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfeature_means\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DatasetGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "p = 2\n",
    "n = 100\n",
    "k = 4\n",
    "\n",
    "DG = DatasetGenerator(p=p, N=n, K=k, class_task=True)\n",
    "\n",
    "feature_means = np.array([[5,5], [-5,-5], [-3,2], [5,-1]])\n",
    "DG.generate_minimal_data(feature_means, np.ones((k,p)))\n",
    "for k_ in range(k):\n",
    "    plt.scatter(DG.X[0, DG.y[k_]==1], DG.X[1, DG.y[k_]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "k = 2\n",
    "DG = DatasetGenerator(p=p, N=n, K=k, class_task=False)\n",
    "\n",
    "feature_means = np.array([5, 5])\n",
    "feature_std = np.ones(p)\n",
    "regression_rule = np.array([[-3, 1]])\n",
    "DG.generate_minimal_data(feature_means, feature_std, regression_rule=regression_rule)\n",
    "\n",
    "for k_ in range(k):\n",
    "    plt.plot(DG.X[0], DG.y[0])    \n",
    "    plt.plot(DG.X[0], DG.y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 2\n",
    "k = 1\n",
    "DG = DatasetGenerator(p=p, N=n, K=k, class_task=False)\n",
    "\n",
    "feature_means = np.array([10, 5])\n",
    "feature_std = np.ones(p)\n",
    "regression_rule = np.array([[-3, 1]])\n",
    "DG.generate_minimal_data(feature_means, feature_std, regression_rule=regression_rule)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(DG.X[0], DG.X[1], DG.y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = DG.add_redundancy(add_p=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t.shape, np.linalg.matrix_rank(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = DG.add_redundancy(A=np.random.randn(3,p))\n",
    "print(X_t.shape)\n",
    "print(np.linalg.matrix_rank(X_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = DG.add_noise(add_p=10)\n",
    "print(X_t.shape, np.linalg.matrix_rank(X_t))\n",
    "print([np.mean(X_t_feat) for X_t_feat in X_t])\n",
    "print([np.std(X_t_feat) for X_t_feat in X_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = DG.add_noise(add_p=10, noise_mean=np.ones(10), noise_std=np.ones(10))\n",
    "print(X_t.shape, np.linalg.matrix_rank(X_t))\n",
    "print([np.mean(X_t_feat) for X_t_feat in X_t])\n",
    "print([np.std(X_t_feat) for X_t_feat in X_t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetGenerator:\n",
    "\n",
    "    def __init__(self,\n",
    "                 p,\n",
    "                 N,\n",
    "                 K,\n",
    "                 class_task=False,\n",
    "                 scenario=1,\n",
    "                 mu_array=None,\n",
    "                 sigma_array=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Generate dataset for a supervised learning task. The features are extracted using\n",
    "        Gaussian distributions.\n",
    "        :param p: int, of relevant features\n",
    "        :param N: int, number of data\n",
    "        :param K: int, output dimension for regression, number of classes for classification task\n",
    "        :param class_task: bool, if False regression, else classification dataset\n",
    "        :param scenario: int, scenario (1,2,4)\n",
    "        :param mu_array: np.array (self.p) of means if regression, otherwise\n",
    "        (self.K, self.p) array for classification\n",
    "        :param sigma_array: np.array (self.p) of standard deviations, otherwise\n",
    "        (self.K, self.p) array for classification\n",
    "        :param regression_rule: only if self.class_task is False, we need a regression law\n",
    "        \"\"\"\n",
    "        self.p = p\n",
    "        self.N = N\n",
    "        self.K = K\n",
    "        self.class_task = class_task\n",
    "        self.scenario = scenario\n",
    "        self.mu_array = mu_array\n",
    "        self.sigma_array = sigma_array\n",
    "\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.minimal_dataset = None\n",
    "        \n",
    "        self.generate_minimal_data()  # we generate the minimal dataset given the hyper-parameters\n",
    "\n",
    "        if scenario is not None:\n",
    "            if 'add_p' not in dct_kwargs.keys():\n",
    "                raise ValueError(\"Specify the amount of additional features through add_p\")\n",
    "            elif dct_kwargs['add_p'] == 0:\n",
    "                return\n",
    "            self.add_p = dct_kwargs['add_p']\n",
    "        \n",
    "        if self.scenario == 1:  # additional noisy features\n",
    "            self.noise_mean = np.array(dct_kwargs['noise_mean']) if 'noise_mean' in dct_kwargs.keys() else 0\n",
    "            self.noise_std = np.array(dct_kwargs['noise_std']) if 'noise_std' in dct_kwargs.keys() else 1\n",
    "            self.add_gaussian_noise()\n",
    "            \n",
    "        elif self.scenario == 2:  # additional redundant features\n",
    "            self.A = np.array(dct_kwargs['A']) if 'A' in dct_kwargs.keys() else None\n",
    "            self.add_redundancy()\n",
    "            \n",
    "        elif self.scenario == 4:  # add noise and redundancy\n",
    "            self.redundancy_amount = dct_kwargs['redundancy_amount'] if 'redundancy_amount' in dct_kwargs.keys() else 0.5\n",
    "            self.A = np.array(dct_kwargs['A']) if 'A' in dct_kwargs.keys() else None\n",
    "            self.noise_mean = np.array(dct_kwargs['noise_mean']) if 'noise_mean' in dct_kwargs.keys() else 0\n",
    "            self.noise_std = np.array(dct_kwargs['noise_std']) if 'noise_std' in dct_kwargs.keys() else 1\n",
    "            # when we call one or the other we automatically save the new self.X_transf\n",
    "            self.add_mixture()\n",
    "        self.dct_kwargs = kwargs\n",
    "\n",
    "\n",
    "    def generate_minimal_data(self):\n",
    "        \"\"\"\n",
    "        Here we generate the data by using the relevant features only.\n",
    "        Each feature is Gaussianly distributed. Mean and standard\n",
    "        deviation for each variable varies depending on the user specification.\n",
    "\n",
    "        The generic i-th feature is x_i\n",
    "                    x_i = mean_i + N(0,1) * std_i, x_i in R^n_samples\n",
    "\n",
    "        The labels are generating depending on the learning task.\n",
    "        If class_task is False, a regression rule is needed, and\n",
    "            y_k = np.dot(regression_rule, x_k)  , x_k in R^n_features,\n",
    "\n",
    "        If class_task is True, the classifier the two distribution are\n",
    "        given different values. # at the moment we are not considering the\n",
    "        multiclassification case.\n",
    "        \"\"\"\n",
    "        if not self.class_task and self.regression_rule is None:  # for regression task\n",
    "            raise ValueError(\"You need a learning rule to generate the regression law.\")\n",
    "\n",
    "        # regression task\n",
    "        if not self.class_task:\n",
    "            self.regression_rule = np.squeeze(self.regression_rule)\n",
    "\n",
    "            if self.regression_rule.ndim == 1:  #  p or K are equal to one\n",
    "                if self.regression_rule.size != self.p and self.regression_rule.size != self.K:\n",
    "                    raise ValueError(\"The regression rule does not match the data dimensionality\")\n",
    "            else:  # if the dimensionality if bigger than one\n",
    "                check_output, check_input = self.regression_rule.shape\n",
    "                if check_output != self.K or check_input != self.p:\n",
    "                    raise ValueError(\"The dimensions for the regression rule do not match the data\")\n",
    "\n",
    "            X_ = np.random.randn(self.p, self.N)  # we generate the new dataset\n",
    "            for id_, (mu_, sigma_) in enumerate(zip(self.mu_array, self.sigma_array)):\n",
    "                X_[id_] *= sigma_\n",
    "                X_[id_] += mu_\n",
    "            self.X = X_\n",
    "            if self.K != 1 and self.p == 1:  # if the number of output != 1\n",
    "                self.regression_rule = self.regression_rule.reshape(-1, 1)\n",
    "            elif self.K == 1 and self.p != 1:\n",
    "                self.regression_rule = self.regression_rule.reshape(-1, )\n",
    "            self.y = np.dot(self.regression_rule, self.X)\n",
    "\n",
    "        # classification task\n",
    "        else:\n",
    "            check_output_mu, check_input_mu = np.squeeze(self.mu_array).shape\n",
    "            check_output_st, check_input_st = np.squeeze(self.sigma_array).shape\n",
    "\n",
    "            if check_output_mu != self.K or check_output_st != self.K:\n",
    "                raise ValueError(\"Arrays inconsistent with the number of classes\")\n",
    "\n",
    "            n_per_class = self.N // self.K\n",
    "\n",
    "            X_ = np.zeros((self.p, self.N))\n",
    "            y_ = np.zeros((self.K, self.N))\n",
    "            for k_, (mu_class_, sigma_class_) in enumerate(zip(self.mu_array, self.sigma_array)):  # for each class\n",
    "                first_ = k_ * n_per_class\n",
    "                last_ = self.N if k_ == self.K - 1 else (k_ + 1) * n_per_class\n",
    "                for id_, (mu_, sigma_) in enumerate(zip(mu_class_, sigma_class_)):\n",
    "                    X_[id_, first_:last_] = mu_ + np.random.randn(last_ - first_) * sigma_\n",
    "                y_[k_, first_:last_] = 1\n",
    "\n",
    "            self.y = y_\n",
    "            self.X = X_\n",
    "        self.minimal_dataset = True\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def add_redundancy(self):\n",
    "        \"\"\" We add redundancy to the dataset.\n",
    "        Using a linear combination of the input features.\n",
    "        \"\"\"\n",
    "        if not self.minimal_dataset:\n",
    "            raise ValueError(\"Generate the dataset first\")\n",
    "\n",
    "        if self.A is not None:\n",
    "            check_add_feat, check_d = self.A.shape  # check the dimensions\n",
    "            if check_d != self.p:\n",
    "                raise ValueError(\"The dimension of the transformation does not check the amount of features\")\n",
    "\n",
    "        else:\n",
    "            self.A = np.random.randn(self.add_p, self.p)  #  add redundancy through a linear transformation\n",
    "\n",
    "        self.X_transf = np.vstack((self.X, self.A.dot(self.X)))  # save as an attribute the feature\n",
    "        \n",
    "    \n",
    "    def add_gaussian_noise(self):\n",
    "        \"\"\" We add noisy features to the dataset.\n",
    "        This is done by adding gaussian distributed\n",
    "        random variables to the original features.\n",
    "        \"\"\"\n",
    "        if not self.minimal_dataset:\n",
    "            raise ValueError(\"Generate the dataset first\")\n",
    "\n",
    "        if np.isscalar(self.noise_mean) and np.isscalar(self.noise_std):\n",
    "            self.X_transf = np.vstack((self.X, (self.noise_mean + \n",
    "                                                self.noise_std * np.random.randn(self.add_p, self.N))))\n",
    "            return\n",
    "        \n",
    "        if self.add_p != self.noise_mean.size and self.add_p != self.noise_std.size:\n",
    "            raise ValueError(\"Mismatch in dimensions\")\n",
    "            \n",
    "        noise_feat = np.zeros((self.add_p, self.N))\n",
    "        for i_, (noise_m_, noise_s_) in enumerate(zip(self.noise_mean, self.noise_std)):\n",
    "            noise_feat[i_] = noise_m_ + noise_s_ * np.random.randn(self.N)\n",
    "        \n",
    "        self.X_transf = np.vstack((self.X, noise_feat))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def add_mixture(self):\n",
    "        \"\"\"With this call we add a percentage of redundancy and a (1-percentage) of noisy features.\n",
    "        \"\"\"\n",
    "        if self.A is None and np.isscalar(self.noise_mean) and np.isscalar(self.noise_std):\n",
    "            n_noise_feat = (self.add_p - int(self.add_p * self.redundancy_amount))\n",
    "            n_rdndt_feat = int(self.add_p * self.redundancy_amount)\n",
    "            self.A = np.random.randn(n_rdndt_feat, self.p)\n",
    "            noise_feat = self.noise_mean + self.noise_std * np.random.randn(n_noise_feat, self.N)\n",
    "\n",
    "        elif self.A is not None:\n",
    "            n_rdndt_feat, p_tmp = self.A.shape\n",
    "            n_noise_feat = self.add_p - n_rdndt_feat\n",
    "            if p_tmp != self.p:\n",
    "                raise ValueError(\"Mismatch in dimensions for the linear transformation\")\n",
    "                \n",
    "            if not np.isscalar(self.noise_mean) and not np.isscalar(self.noise_std):\n",
    "                noise_feat = np.zeros((self.noise_mean.size, self.N))\n",
    "                if self.noise_mean.size != self.noise_std.size:  # check that the shapes are consistent\n",
    "                    raise ValueError(\"The noise values are inconsistent in shape\")\n",
    "                n_noise_feat = self.noise_mean.size\n",
    "                # check not to exceed the add_p value\n",
    "                            \n",
    "                if self.add_p != (n_noise_feat + n_rdndt_feat):\n",
    "                    raise ValueError(\"The sum of the two dimensions is different from the specified number of additional features\")\n",
    "                for i_, (noise_m_, noise_s_) in enumerate(zip(self.noise_mean, self.noise_std)):\n",
    "                    noise_feat[i_] = noise_m_ + noise_s_ * np.random.randn(self.N)\n",
    "            else:\n",
    "                # tested\n",
    "                noise_feat = self.noise_mean + self.noise_std * np.random.randn(n_noise_feat, self.N)\n",
    "            \n",
    "        X_r = np.vstack((self.X, np.dot(self.A, self.X)))\n",
    "        self.noise = noise_feat\n",
    "        \n",
    "        self.X_transf = np.vstack((X_r, noise_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_array = np.array([[5,5], [-5,-5], [-3,2], [5,-1]])\n",
    "sigma_array = np.ones((4,2))\n",
    "p = 2\n",
    "n = 100\n",
    "k = 4\n",
    "add_p = 10 \n",
    "\n",
    "DataGen = DatasetGenerator(p=p,N=n,K=k,class_task=True,\n",
    "                           scenario=2, mu_array=mu_array,\n",
    "                           sigma_array=sigma_array,\n",
    "                           regression_rule=1,\n",
    "                           add_p=add_p,\n",
    "                           A=list(np.random.randn(add_p, p)))\n",
    "\n",
    "for k_ in range(k):\n",
    "    plt.scatter(DataGen.X[0, DataGen.y[k_]==1], \n",
    "                DataGen.X[1, DataGen.y[k_]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_array = np.array([[5,5], [-5,-5], [-3,2], [5,-1]])\n",
    "sigma_array = np.ones((4,2))\n",
    "p = 2\n",
    "n = 100\n",
    "k = 4\n",
    "add_p = 10 \n",
    "\n",
    "DataGen = DatasetGenerator(p=p,N=n,K=k,class_task=True,\n",
    "                           scenario=1, mu_array=mu_array,\n",
    "                           sigma_array=sigma_array,\n",
    "                           add_p=add_p,\n",
    "                           noise_mean=list(np.zeros(add_p)),\n",
    "                           noise_std=list(3*np.ones(add_p)))\n",
    "\n",
    "for k_ in range(k):\n",
    "    plt.scatter(DataGen.X[0, DataGen.y[k_]==1], \n",
    "                DataGen.X[1, DataGen.y[k_]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if you satify all the required conditions\n",
    "\n",
    "corrupted_dataset = DataGen.X_transf\n",
    "print(np.linalg.matrix_rank(corrupted_dataset))\n",
    "\n",
    "mean_ = np.mean(corrupted_dataset, axis=-1)\n",
    "std_ = np.std(corrupted_dataset, axis=-1)\n",
    "for (m_, s_) in zip(mean_, std_):\n",
    "    print(m_, s_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_array = np.array([[5,5], [-5,-5], [-3,2], [5,-1]])\n",
    "sigma_array = np.ones((4,2))\n",
    "p = 2\n",
    "n = 100\n",
    "k = 4\n",
    "add_p = 10 \n",
    "\n",
    "DataGen = DatasetGenerator(p=p,N=n,K=k,class_task=True,\n",
    "                           scenario=4, mu_array=mu_array,\n",
    "                           sigma_array=sigma_array,\n",
    "                           add_p=add_p,\n",
    "                           redundancy_amount=0.25,\n",
    "                           A=list(np.random.randn(3,p)),\n",
    "                           noise_mean=list(np.array([2,3,4,5,1,1,7])),\n",
    "                           noise_std=list(np.ones(7)))\n",
    "\n",
    "for k_ in range(k):\n",
    "    plt.scatter(DataGen.X[0, DataGen.y[k_]==1], \n",
    "                DataGen.X[1, DataGen.y[k_]==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataGen.A, DataGen.noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(DataGen.noise, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(DataGen.noise, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_kwargs = {'add_p': 10,\n",
    "              'redundancy_amount': 0.25,\n",
    "              'A': list(np.random.randn(3,p)), \n",
    "              'noise_mean': list(np.array([2,3,4,5,1,1,7])),\n",
    "              'noise_std': list(np.ones(7))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_kwargs = {'add_p': 0,\n",
    "              'redundancy_amount': 0.25,\n",
    "              'A': list(np.random.randn(3,p)), \n",
    "              'noise_mean': list(np.array([2,3,4,5,1,1,7])),\n",
    "              'noise_std': list(np.ones(7))}\n",
    "\n",
    "DataGen = DatasetGenerator(p=p,N=n,K=k,class_task=True,\n",
    "                           scenario=4, mu_array=mu_array,\n",
    "                           sigma_array=sigma_array,\n",
    "                           **dct_kwargs)\n",
    "\n",
    "DataGen.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 3\n",
    "k = 2\n",
    "\n",
    "dct_kwargs = {'add_p': 0,\n",
    "              'redundancy_amount': 0.25,\n",
    "              'A': list(np.random.randn(3,p)), \n",
    "              'noise_mean': list(np.array([2,3,4,5,1,1,7])),\n",
    "              'noise_std': list(np.ones(7))}\n",
    "\n",
    "mu_array = np.array([[1,2,3],[2,1,1]])\n",
    "sigma_array = 0.5 * np.ones((k,p))\n",
    "\n",
    "X_splits = []\n",
    "y_splits = []\n",
    "for n_ in [50, 100, 100]:\n",
    "    DataGen = DatasetGenerator(p=p,N=n,K=k,class_task=True,\n",
    "                               scenario=4, mu_array=mu_array,\n",
    "                               sigma_array=sigma_array,\n",
    "                               **dct_kwargs)\n",
    "\n",
    "    X_splits.append(DataGen.X.T)\n",
    "    y_splits.append(DataGen.y.T)\n",
    "    \n",
    "X_tr, X_vl, X_ts = X_splits\n",
    "y_tr, y_vl, y_ts = y_splits\n",
    "\n",
    "del X_splits, y_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 3), (100, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dct = {'square_loss': 'mean_squared_error',\n",
    "           'cross_entropy': 'categorical_crossentropy'}\n",
    "\n",
    "nodes = 128\n",
    "output_dims = 2\n",
    "loss = 'square_loss'\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(Dense(nodes, input_shape=(p,), activation='linear'))  # n_params = nodes + bias\n",
    "    \n",
    "if loss == 'cross_entropy':\n",
    "    model.add(Dense(output_dims,\n",
    "                   activation='softmax'))\n",
    "elif loss == 'square_loss':\n",
    "    model.add(Dense(output_dims,\n",
    "                    activation='softmax'))\n",
    "    # should we put this activation for the square loss also?\n",
    "    \n",
    "sgd = SGD(lr=0.1, momentum=0., nesterov=False)\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                                 patience=5, min_lr=0)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, min_delta=1e-6)\n",
    "\n",
    "model.compile(optimizer=sgd,\n",
    "              loss=loss_dct[loss],\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 3), (100, 2))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape, y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples\n",
      "Epoch 1/20\n",
      "100/100 [==============================] - 0s 4ms/sample - loss: 0.2245 - accuracy: 0.6000\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 0s 109us/sample - loss: 0.1227 - accuracy: 0.8100\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 0s 391us/sample - loss: 0.0699 - accuracy: 0.9400\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 0s 283us/sample - loss: 0.0515 - accuracy: 0.9900\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 0s 439us/sample - loss: 0.0429 - accuracy: 0.9900\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 0s 774us/sample - loss: 0.0377 - accuracy: 0.9900\n",
      "Epoch 7/20\n",
      "100/100 [==============================] - 0s 300us/sample - loss: 0.0343 - accuracy: 0.9900\n",
      "Epoch 8/20\n",
      "100/100 [==============================] - 0s 223us/sample - loss: 0.0302 - accuracy: 0.9900\n",
      "Epoch 9/20\n",
      "100/100 [==============================] - 0s 200us/sample - loss: 0.0281 - accuracy: 0.9900\n",
      "Epoch 10/20\n",
      "100/100 [==============================] - 0s 123us/sample - loss: 0.0280 - accuracy: 0.9900\n",
      "Epoch 11/20\n",
      "100/100 [==============================] - 0s 224us/sample - loss: 0.0249 - accuracy: 0.9900\n",
      "Epoch 12/20\n",
      "100/100 [==============================] - 0s 159us/sample - loss: 0.0237 - accuracy: 0.9900\n",
      "Epoch 13/20\n",
      "100/100 [==============================] - 0s 221us/sample - loss: 0.0227 - accuracy: 0.9900\n",
      "Epoch 14/20\n",
      "100/100 [==============================] - 0s 122us/sample - loss: 0.0250 - accuracy: 0.9800\n",
      "Epoch 15/20\n",
      "100/100 [==============================] - 0s 223us/sample - loss: 0.0215 - accuracy: 0.9900\n",
      "Epoch 16/20\n",
      "100/100 [==============================] - 0s 158us/sample - loss: 0.0214 - accuracy: 0.9800\n",
      "Epoch 17/20\n",
      "100/100 [==============================] - 0s 206us/sample - loss: 0.0190 - accuracy: 0.9900\n",
      "Epoch 18/20\n",
      "100/100 [==============================] - 0s 172us/sample - loss: 0.0189 - accuracy: 0.9900\n",
      "Epoch 19/20\n",
      "100/100 [==============================] - 0s 191us/sample - loss: 0.0180 - accuracy: 0.9900\n",
      "Epoch 20/20\n",
      "100/100 [==============================] - 0s 220us/sample - loss: 0.0177 - accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_tr, y_tr, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.224550</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122705</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.069901</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.051538</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042918</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.037735</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.034322</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.030158</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.028148</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.027971</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.024858</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.023696</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.022721</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.024979</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.021453</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.019011</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.018889</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.018007</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  accuracy\n",
       "0   0.224550      0.60\n",
       "1   0.122705      0.81\n",
       "2   0.069901      0.94\n",
       "3   0.051538      0.99\n",
       "4   0.042918      0.99\n",
       "5   0.037735      0.99\n",
       "6   0.034322      0.99\n",
       "7   0.030158      0.99\n",
       "8   0.028148      0.99\n",
       "9   0.027971      0.99\n",
       "10  0.024858      0.99\n",
       "11  0.023696      0.99\n",
       "12  0.022721      0.99\n",
       "13  0.024979      0.98\n",
       "14  0.021453      0.99\n",
       "15  0.021429      0.98\n",
       "16  0.019011      0.99\n",
       "17  0.018889      0.99\n",
       "18  0.018007      0.99\n",
       "19  0.017672      0.99"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "?? np.minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataGen.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "def compute_pinv(x_tr, y_tr, data_gen):\n",
    "    \"\"\" pinv for the different cases\n",
    "    x_tr must be of size (N, #features)\n",
    "    y_tr must be of size (N, #classes) \"\"\"\n",
    "    n_training, input_dimensions = X.shape\n",
    "    reduce_dims = False\n",
    "    original_dims = data_gen.p \n",
    "    dct_kwargs = data_gen.dct_kwargs\n",
    "    dct_kwargs['A'] = np.array(dct_kwargs['A'])\n",
    "    \n",
    "    if scenario == 1:\n",
    "        if input_dimensions > n_training:\n",
    "            # n < p, ill-posed problem\n",
    "            inv_val = inv(np.dot(x_tr, x_tr.T))  # this is n x n\n",
    "            pseudoinv = np.dot(x_tr.T, np.dot(inv_val, y_tr))\n",
    "        else:\n",
    "            inv_val = inv(np.dot(x_tr.T, x_tr))  # this is p x p\n",
    "            pseudoinv = np.dot(inv_val, np.dot(x_tr.T, y_tr))\n",
    "\n",
    "    elif scenario == 2:\n",
    "        F = np.vstack((np.identity(original_dims),\n",
    "                       dct_kwargs['A']))\n",
    "\n",
    "        if n_training <= original_dims:\n",
    "            frame_prod = np.dot(F.T, F)\n",
    "            inv_val = inv(np.dot(np.dot(x_tr[:,:original_dims],\n",
    "                                        frame_prod),\n",
    "                                        x_tr[:, :original_dims].T))\n",
    "            pseudoinv = np.dot(F.dot(x_tr[:, :original_dims].T),\n",
    "                               np.dot(inv_val, y_tr))\n",
    "        else:\n",
    "            # this is wF\n",
    "            pseudoinv = np.dot(inv(np.dot(x_tr[:, :original_dims].T,\n",
    "                                          x_tr[:, :original_dims])),\n",
    "                               np.dot(x_tr[:, :original_dims].T, y_tr))\n",
    "            reduce_dims = True\n",
    "\n",
    "    elif scenario == 4:\n",
    "        dims_redundant_, _ = dct_kwargs['A'].shape\n",
    "        dims_noise_ = input_dimensions-(dims_redundant_ + original_dims)\n",
    "        x_tr_ = x_tr[:, :input_dimensions-dims_redundant_]\n",
    "        lw_part = np.hstack((dct_kwargs['A'],\n",
    "                             np.zeros((dims_redundant_, dims_noise_))))\n",
    "        F = np.vstack((np.identity(dims_noise_ + original_dims),\n",
    "                       lw_part))\n",
    "        if n_training <= input_dimensions-dims_redundant_:\n",
    "            inv_val = inv(np.dot(x_tr_, np.dot(np.dot(F.T, F), x_tr_.T)))\n",
    "            pseudoinv = np.dot(np.dot(F.dot(x_tr_.T), inv_val), y_tr)\n",
    "\n",
    "        else:  # this is FTW\n",
    "            pseudoinv = np.dot(inv(np.dot(x_tr_.T, x_tr_)), np.dot(x_tr_.T, y_tr))\n",
    "            reduce_dims = True\n",
    "    return pseudoinv, reduce_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noise 16\n",
      "redundant 4\n",
      "(100, 23) (100, 2)\n"
     ]
    }
   ],
   "source": [
    "p = 3\n",
    "k = 2\n",
    "n = 100\n",
    "add_p = 20\n",
    "scenario = 4\n",
    "redundancy_amount = 0.20\n",
    "\n",
    "print('noise', int((1-redundancy_amount)*add_p))\n",
    "print('redundant', int(add_p * redundancy_amount))\n",
    "\n",
    "dct_kwargs = {'add_p': add_p,\n",
    "              'redundancy_amount': 0.25,\n",
    "              'A': list(np.random.randn(int(add_p * redundancy_amount), p)), \n",
    "              'noise_mean': list(np.ones(int((1-redundancy_amount)*add_p))),\n",
    "              'noise_std': list(np.ones(int((1-redundancy_amount)*add_p)))}\n",
    "\n",
    "mu_array = np.array([[1,2,3],[2,1,1]]) # np.vstack((np.arange(p), -np.arange(p)))  # \n",
    "sigma_array = 0.5 * np.ones((k,p))\n",
    "\n",
    "DataGen = DatasetGenerator(p=p,N=n,K=k,class_task=True,\n",
    "                           scenario=scenario, mu_array=mu_array,\n",
    "                           sigma_array=sigma_array,\n",
    "                           **dct_kwargs)\n",
    "X = DataGen.X_transf.T\n",
    "y = DataGen.y.T\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "sol, reduce_flag = compute_pinv(X, y, DataGen)\n",
    "print(reduce_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.78894994,  0.63720865],\n",
       "       [ 0.99129518, -2.18313463],\n",
       "       [ 0.19475716,  1.22927215],\n",
       "       [ 0.69974808,  1.68275298],\n",
       "       [ 1.2434664 , -0.01802449],\n",
       "       [-0.32192766,  0.03981197],\n",
       "       [-0.39756698,  0.46471529],\n",
       "       [-1.03282662,  0.80397278],\n",
       "       [-0.50260782,  0.96825997],\n",
       "       [-0.70430505,  1.39876852]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(10,2)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "[[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "y_ = np.zeros_like(x).astype('int')\n",
    "print(y_)\n",
    "\n",
    "y_[np.arange(y_.shape[0]), np.argmax(x, axis=-1)]=1\n",
    "print(y_)\n",
    "\n",
    "y_true = y_.copy()\n",
    "y_true[0] = np.array([0,1])\n",
    "\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array([np.dot(y_t, y_p) for (y_t, y_p) in zip(y_true, y_)])) / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.dot(y_t, y_p) for (y_t, y_p) in zip(y_true, y_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1]]), array([[1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1]]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
